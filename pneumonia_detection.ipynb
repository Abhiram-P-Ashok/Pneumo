{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Class weight calculation\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Keras library\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Different CNN Model\n",
    "from keras.applications import VGG16, InceptionV3, MobileNetV2, DenseNet121\n",
    "\n",
    "# To chain two different data augmented images for training\n",
    "from itertools import chain\n",
    "\n",
    "#  Distributed Computing\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "\n",
    "image_height = 299\n",
    "image_width = 299\n",
    "\n",
    "# Data agumentation and pre-processing using tensorflow\n",
    "data_generator_1 = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            rotation_range=5,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            shear_range=0.05,\n",
    "                            zoom_range=0.05,\n",
    "                            brightness_range = [0.95,1.05],\n",
    "                            horizontal_flip=False,\n",
    "                            vertical_flip=False,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "print('Data Augmentation 1 was created')\n",
    "\n",
    "data_generator_2 = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            brightness_range = [0.9,1.1],\n",
    "                            horizontal_flip=False,\n",
    "                            vertical_flip=False,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "print('Data Augmentation 2 was created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_3 = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator1 = data_generator_1.flow_from_directory(\n",
    "    directory=r\"C:\\Users\\PC\\OneDrive\\Desktop\\pneumonia_detection\\data\\data\\Train\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size=(image_height, image_width),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('Data Augmentation 1 was used to generate the train data set\\n')\n",
    "\n",
    "test_generator = data_generator_3.flow_from_directory(\n",
    "    directory=r\"C:\\Users\\PC\\OneDrive\\Desktop\\pneumonia_detection\\data\\data\\Test\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size=(image_height, image_width),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class = train_generator1.class_indices\n",
    "print('Dictionary: {}'.format(dict_class))\n",
    "class_names = list(dict_class.keys())  # storing class/breed names in a list\n",
    "print('Class labels: {}'.format(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = np.unique(train_generator1.classes, return_counts=True)\n",
    "\n",
    "plt.title(\"Trainning dataset\", fontsize='16')\n",
    "plt.pie(frequency[1], labels = class_names, autopct='%1.0f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset characteristics\n",
    "print(\"Dataset Characteristics of Train Data Set:\")\n",
    "print(\"Number of images:\", len(train_generator1.classes))\n",
    "print(\"Number of bacterial pneumonia images:\", len([label for label in train_generator1.classes if label == 0]))\n",
    "print(\"Number of normal images:\", len([label for label in train_generator1.classes if label == 1]))\n",
    "print(\"Number of viral pneumonia images:\", len([label for label in train_generator1.classes if label == 2]))\n",
    "print()\n",
    "\n",
    "print(\"Dataset Characteristics of Test Data Set:\")\n",
    "print(\"Number of images:\", len(test_generator.classes))\n",
    "print(\"Number of bacterial pneumonia images:\", len([label for label in test_generator.classes if label == 0]))\n",
    "print(\"Number of normal images:\", len([label for label in test_generator.classes if label == 1]))\n",
    "print(\"Number of viral pneumonia images:\", len([label for label in test_generator.classes if label == 2]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_generator1.classes), y= train_generator1.classes)\n",
    "class_weights = dict(zip(np.unique(train_generator1.classes), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train image data from Data Augmentation 1')\n",
    "img, label = next(train_generator1)\n",
    "# print(len(label))\n",
    "\n",
    "plt.figure(figsize=[10, 5])\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(img[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[np.argmax(label[i])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the epochs for training\n",
    "EPOCHS = 4\n",
    "\n",
    "# Define the number of GPUs to use\n",
    "num_gpus = 2\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.001, patience=10, verbose=1)\n",
    "\n",
    "train_data = train_generator1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope\n",
    "with strategy.scope():\n",
    "\n",
    "    # Load the pre-trained VGG16 model without the top classification layer\n",
    "    base_model_VGG16 = VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
    "\n",
    "    # Set the layers of the base model as non-trainable (freeze them)\n",
    "    for layer in base_model_VGG16.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create a new model and add the VGG16 base model\n",
    "    model_VGG16 = Sequential()\n",
    "    model_VGG16.add(base_model_VGG16)\n",
    "\n",
    "    # Add a fully connected layer and output layer for classification\n",
    "    model_VGG16.add(GlobalAveragePooling2D())\n",
    "    model_VGG16.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_VGG16.add(Dropout(0.4))\n",
    "    model_VGG16.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_VGG16.add(Dropout(0.2))\n",
    "    model_VGG16.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Model summary\n",
    "    print(\"Model Summary (VGG16):\")\n",
    "    model_VGG16.summary()\n",
    "    print()\n",
    "\n",
    "    # Compile the model\n",
    "    model_VGG16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history_VGG16 = model_VGG16.fit(train_data, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss_VGG16, val_accuracy_VGG16 = model_VGG16.evaluate(test_generator, steps=len(test_generator))\n",
    "    print(f'Validation Loss: {val_loss_VGG16:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy_VGG16:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope\n",
    "with strategy.scope():\n",
    "\n",
    "    # Load the pre-trained MobileNetV2 model without the top classification layer\n",
    "    base_model_MobileNet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
    "\n",
    "    # Set the layers of the base model as non-trainable (freeze them)\n",
    "    for layer in base_model_MobileNet.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create a new model and add the MobileNetV2 base model\n",
    "    model_MobileNet = Sequential()\n",
    "    model_MobileNet.add(base_model_MobileNet)\n",
    "\n",
    "    # Add a global average pooling layer and output layer for classification\n",
    "    model_MobileNet.add(GlobalAveragePooling2D())\n",
    "    model_MobileNet.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_MobileNet.add(Dropout(0.4))\n",
    "    model_MobileNet.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_MobileNet.add(Dropout(0.2))\n",
    "    model_MobileNet.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Model summary\n",
    "    print(\"Model Summary (MobileNetV2):\")\n",
    "    model_MobileNet.summary()\n",
    "    print()\n",
    "\n",
    "    # Compile the model\n",
    "    model_MobileNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history_MobileNet = model_MobileNet.fit(train_data, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss_MobileNet, val_accuracy_MobileNet = model_MobileNet.evaluate(test_generator, steps=len(test_generator))\n",
    "    print(f'Validation Loss: {val_loss_MobileNet:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy_MobileNet:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope\n",
    "with strategy.scope():\n",
    "\n",
    "    # Load the pre-trained DenseNet121 model without the top classification layer\n",
    "    base_model_DenseNet = DenseNet121(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
    "\n",
    "    # Set the layers of the base model as non-trainable (freeze them)\n",
    "    for layer in base_model_DenseNet.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create a new model and add the DenseNet121 base model\n",
    "    model_DenseNet = Sequential()\n",
    "    model_DenseNet.add(base_model_DenseNet)\n",
    "\n",
    "    # Add a global average pooling layer and output layer for classification\n",
    "    model_DenseNet.add(GlobalAveragePooling2D())\n",
    "    model_DenseNet.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_DenseNet.add(Dropout(0.4))\n",
    "    model_DenseNet.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_DenseNet.add(Dropout(0.2))\n",
    "    model_DenseNet.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Model summary\n",
    "    print(\"Model Summary (DenseNet121):\")\n",
    "    model_DenseNet.summary()\n",
    "    print()\n",
    "\n",
    "    # Compile the model\n",
    "    model_DenseNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history_DenseNet = model_DenseNet.fit(train_data, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss_DenseNet, val_accuracy_DenseNet = model_DenseNet.evaluate(test_generator, steps=len(test_generator))\n",
    "    print(f'Validation Loss: {val_loss_DenseNet:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy_DenseNet:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope\n",
    "with strategy.scope():\n",
    "\n",
    "    # Load the pre-trained InceptionV3 model without the top classification layer\n",
    "    base_model_Inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
    "\n",
    "    # Set the layers of the base model as non-trainable (freeze them)\n",
    "    for layer in base_model_Inception.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create a new model and add the InceptionV3 base model\n",
    "    model_Inception = Sequential()\n",
    "    model_Inception.add(base_model_Inception)\n",
    "\n",
    "    # Add a global average pooling layer and output layer for classification\n",
    "    model_Inception.add(GlobalAveragePooling2D())\n",
    "    model_Inception.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_Inception.add(Dropout(0.4))\n",
    "    model_Inception.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model_Inception.add(Dropout(0.2))\n",
    "\n",
    "    model_Inception.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Model summary\n",
    "    print(\"Model Summary (InceptionV3):\")\n",
    "    model_Inception.summary()\n",
    "    print()\n",
    "\n",
    "    # Compile the model\n",
    "    model_Inception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with EarlyStopping\n",
    "    history_Inception = model_Inception.fit(train_data, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss_Inception, val_accuracy_Inception = model_Inception.evaluate(test_generator, steps=len(test_generator))\n",
    "    print(f'Validation Loss: {val_loss_Inception:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy_Inception:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'VGG16': val_accuracy_VGG16,\n",
    "    'MobileNet': val_accuracy_MobileNet,\n",
    "    'DenseNet': val_accuracy_DenseNet,\n",
    "    'Inception': val_accuracy_Inception\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index', columns=['accuracy'])\n",
    "df = df.reset_index().rename(columns={'index': 'model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart\n",
    "sns.barplot(x='model', y='accuracy', data=df)\n",
    "\n",
    "# Add labels to bars\n",
    "ax = plt.gca()\n",
    "for bar in ax.containers:\n",
    "    ax.bar_label(bar, label_type='edge', labels=[f\"{x:.1%}\" for x in bar.datavalues], fontsize=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model with frozen layers\n",
    "\n",
    "# Unfreeze the base_model\n",
    "no_base_layers = len(model_MobileNet.layers)\n",
    "print('No. of base layers in the model = {}\\n'.format(no_base_layers))\n",
    "\n",
    "# no of layers for fine-tune\n",
    "no_finetune_layers = int(no_base_layers/2)\n",
    "print('No. of layers for fine-tune = {}'.format(no_finetune_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "model_MobileNet.trainable = True\n",
    "for layer in model_MobileNet.layers[: -no_finetune_layers]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# small learning rate for fine tuning\n",
    "model_MobileNet.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_finetune_MobileNet = model_MobileNet.fit(train_data, epochs=EPOCHS, validation_data=test_generator, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "print('====== Model Validation ======')\n",
    "val_loss_finetune_MobileNet, val_accuracy_finetune_MobileNet = model_MobileNet.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f'Validation Loss: {val_loss_finetune_MobileNet:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy_finetune_MobileNet:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions to allow computation of the confusion matrix later on\n",
    "# Do not shuffle predictions otherwise we have no way of finding out the true labels\n",
    "MobileNet_test_preds = model_MobileNet.predict(test_generator)\n",
    "MobileNet_test_pred_classes = np.argmax(MobileNet_test_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "img, label = next(test_generator)\n",
    "\n",
    "prediction = model_MobileNet.predict(img)\n",
    "test_pred_classes = np.argmax(prediction, axis=1)\n",
    "\n",
    "plt.figure(figsize=[14, 14])\n",
    "for i in range(20):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    plt.imshow(img[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label : {}\\n Prediction : {} {:.1f}%\".format(class_names[np.argmax(label[i])], class_names[test_pred_classes[i]], 100 * np.max(prediction[i])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r\"C:\\Users\\PC\\OneDrive\\Desktop\\pneumonia detection\\data\\data\\Test\"\n",
    "\n",
    "# Image dimensions\n",
    "image_height = 299\n",
    "image_width = 299\n",
    "\n",
    "# Create ImageDataGenerator for test data\n",
    "test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load test data using the generator\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=1,\n",
    "    shuffle=False,  # Important: Set shuffle to False to maintain order\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get predictions and true labels\n",
    "y_pred = model_MobileNet.predict(test_generator, steps=len(test_generator))\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "file_names = test_generator.filenames\n",
    "\n",
    "# Identify mispredicted indices\n",
    "misclassified_indices = np.where(y_pred_classes != y_true)[0]\n",
    "\n",
    "# Print file names of mispredicted images\n",
    "print(\"\\nMispredicted File Names:\")\n",
    "for idx in misclassified_indices:\n",
    "    print(file_names[idx])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Classification Report\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "class_report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
    "\n",
    "# Display Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Display Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Save model architecture to JSON file\n",
    "model_json = model_MobileNet.to_json()\n",
    "with open(\"Mobile_classifier.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to HDF5 file\n",
    "model_MobileNet.save_weights(\"Mobile_classifier_weights.h5\")\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
